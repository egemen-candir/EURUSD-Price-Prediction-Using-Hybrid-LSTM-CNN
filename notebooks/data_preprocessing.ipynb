{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# API Key\n",
    "api_key = '816dce360a1c40c7abd7bcc055561bf0'\n",
    "\n",
    "# Initialize variables\n",
    "symbol = 'EUR/USD'\n",
    "interval = '15min'\n",
    "start_date = '2014-01-01'\n",
    "end_date = '2024-01-01'\n",
    "batch_size = 45  # Number of days per batch\n",
    "\n",
    "# Function to fetch data for a specific date range\n",
    "def fetch_data(start, end):\n",
    "    url = f'https://api.twelvedata.com/time_series?apikey={api_key}&symbol={symbol}&interval={interval}&start_date={start}&end_date={end}&fmt=json'\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "# Initialize DataFrame to hold all data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop through 2-year batches\n",
    "for year in range(2014, 2024, 2):\n",
    "    current_start = pd.to_datetime(f'{year}-01-01')\n",
    "    current_end = current_start + pd.DateOffset(years=2)\n",
    "    \n",
    "    if current_end > pd.to_datetime(end_date):\n",
    "        current_end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Fetch data in smaller batches to avoid API limits\n",
    "    while current_start < current_end:\n",
    "        batch_end = current_start + pd.DateOffset(days=batch_size)\n",
    "        if batch_end > current_end:\n",
    "            batch_end = current_end\n",
    "        \n",
    "        data = fetch_data(current_start.strftime('%Y-%m-%d'), batch_end.strftime('%Y-%m-%d'))\n",
    "        if 'values' in data:\n",
    "            df = pd.DataFrame(data['values'])\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "            print(f\"Fetched data from {current_start.strftime('%Y-%m-%d')} to {batch_end.strftime('%Y-%m-%d')}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {data}\")\n",
    "            break\n",
    "        \n",
    "        current_start = batch_end\n",
    "        # Pause to avoid API rate limits\n",
    "        time.sleep(300)  # 5-minute pause\n",
    "\n",
    "# Save data to CSV file\n",
    "all_data.to_csv('eurusd_15min_data.csv', index=False)\n",
    "print(f\"Data saved to eurusd_15min_data.csv\")\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "all_data = pd.read_csv('eurusd_15min_data.csv')\n",
    "\n",
    "# Print columns to verify available features\n",
    "print(\"Available columns:\", all_data.columns.tolist())\n",
    "\n",
    "# Convert datetime to pandas datetime and set as index\n",
    "all_data['datetime'] = pd.to_datetime(all_data['datetime'])\n",
    "all_data.set_index('datetime', inplace=True)\n",
    "all_data = all_data.sort_index()\n",
    "\n",
    "# Display the number of data points\n",
    "num_data_points = len(all_data)\n",
    "print(f\"Number of data points: {num_data_points}\")\n",
    "\n",
    "# Calculate technical indicators\n",
    "all_data['SMA_5'] = all_data['close'].rolling(window=5).mean()\n",
    "all_data['SMA_20'] = all_data['close'].rolling(window=20).mean()\n",
    "all_data['RSI'] = calculate_rsi(all_data['close'], periods=14)\n",
    "all_data['MACD'] = calculate_macd(all_data['close'])\n",
    "all_data['ATR'] = calculate_atr(all_data[['high', 'low', 'close']], period=14)\n",
    "all_data['Momentum'] = calculate_momentum(all_data)\n",
    "\n",
    "# Drop NaN values\n",
    "all_data.dropna(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
